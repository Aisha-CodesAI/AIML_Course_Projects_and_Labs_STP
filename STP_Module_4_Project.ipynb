{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4: Linear Classifiers & Gradient Descent"
      ],
      "metadata": {
        "id": "xET1TacFQWQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case Study: Predictive Modeling for Public Water Safety**\n",
        "\n",
        "**Objective:** Develop a robust classifier to identify potable water samples. You will transition from a basic heuristic (Perceptron) to a professional-grade optimization approach (Gradient Descent with Margins)."
      ],
      "metadata": {
        "id": "_V7gSkZHQXdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Acquisition & Cleaning\n",
        "\n",
        "In real-world data science, datasets are rarely perfect. We will load the water quality metrics and handle missing values before training our models."
      ],
      "metadata": {
        "id": "YjRDChliR1BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset from a public raw GitHub URL\n",
        "url = \"https://raw.githubusercontent.com/nferran/tp_aprendizaje_de_maquina_I/main/water_potability.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 1: Handling Missing Values\n",
        "# Water sensors often fail, leaving NaNs. We will fill them with the mean of the column.\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "# Step 2: Feature Selection & Labeling\n",
        "# We'll use all chemical features to predict 'Potability'\n",
        "X = df.drop('Potability', axis=1).values\n",
        "y = df['Potability'].values\n",
        "\n",
        "# Step 3: Class Label Conversion\n",
        "# Many linear classifiers (like Perceptron/SVM) require labels to be -1 and 1\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "# Step 4: Train-Test Split & Scaling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Dataset Loaded: {X_train.shape[0]} training samples, {X_train.shape[1]} features.\")"
      ],
      "metadata": {
        "id": "LOg9j_w7R8uU",
        "outputId": "2151972a-f337-4cae-b299-ea05ab0505a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded: 2620 training samples, 9 features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Phase 1: The Heuristic Approach (Perceptron)\n",
        "\n",
        "The **Perceptron** represents the earliest form of supervised learning. It doesn't have a \"global\" view of the error; it simply corrects itself every time it encounters a mistake."
      ],
      "metadata": {
        "id": "_Mr2gopTSBjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Implement the Perceptron Update Rule inside the training loop."
      ],
      "metadata": {
        "id": "h16OQo9LSLDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WaterPerceptron:\n",
        "    def __init__(self, lr=0.01, epochs=50):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "        self.mistakes = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        for epoch in range(self.epochs):\n",
        "            count = 0\n",
        "            for i in range(len(y)):\n",
        "\n",
        "\n",
        "                prediction = np.dot(X[i], self.w) + self.b\n",
        "\n",
        "                if y[i] * prediction <= 0:\n",
        "                    self.w = self.w + self.lr * y[i] * X[i]\n",
        "                    self.b = self.b + self.lr * y[i]\n",
        "                    count += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            self.mistakes.append(count)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)\n",
        "\n",
        "# model_p = WaterPerceptron()\n",
        "# model_p.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "mqxF5SQGSA8-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Perceptron\n",
        "perceptron = WaterPerceptron()\n",
        "perceptron.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = perceptron.predict(X_test)\n",
        "\n",
        "print(\"Predictions:\", y_pred[:10])\n",
        "print(\"Actual:     \", y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXsJZ-cfrBfG",
        "outputId": "8f367995-7ff3-402b-dc39-5530f72e090d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [-1. -1.  1. -1. -1. -1. -1. -1.  1.  1.]\n",
            "Actual:      [-1  1 -1 -1  1  1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Phase 2: Gradient Descent - Global Optimization\n",
        "\n",
        "The Perceptron is unstable if the data isn't perfectly separable. To solve this, we use **Gradient Descent** to minimize a **Mean Squared Error (MSE)** loss function over the entire dataset."
      ],
      "metadata": {
        "id": "Xzzq_ziOSQmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task:** Implement the batch gradient calculation for weights and bias."
      ],
      "metadata": {
        "id": "xJzJuR77SRiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GDWaterClassifier:\n",
        "    def __init__(self, lr=0.001, epochs=500):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "        self.cost_history = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        n = X.shape[0]\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            # 1. Linear output\n",
        "            z = np.dot(X, self.w) + self.b\n",
        "\n",
        "            # 2. Gradients\n",
        "            dw = (1 / n) * np.dot(X.T, (z - y))\n",
        "            db = (1 / n) * np.sum(z - y)\n",
        "\n",
        "            # 3. Update weights and bias\n",
        "            self.w = self.w - self.lr * dw\n",
        "            self.b = self.b - self.lr * db\n",
        "\n",
        "\n",
        "            cost = (1 / (2 * n)) * np.sum((z - y) ** 2)\n",
        "            self.cost_history.append(cost)\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)"
      ],
      "metadata": {
        "id": "Q9iFCaG3Se2L"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "gd_model = GDWaterClassifier()\n",
        "gd_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = gd_model.predict(X_test)\n",
        "\n",
        "# Print first 10 predictions\n",
        "print(\"Predictions:\", y_pred[:10])\n",
        "print(\"Actual:     \", y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l097l7ekqktN",
        "outputId": "04c8cb46-8a06-4942-f099-434ced1e3f2d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [-1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "Actual:      [-1  1 -1 -1  1  1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Phase 3: Margin Classifiers & Hinge Loss\n",
        "\n",
        "In water safety, we aim for more than just correctness—we want a **Margin**, a safety gap between safe and unsafe samples. This is achieved using **Hinge Loss** combined with **L2 Regularization**.\n",
        "\n",
        "The loss function is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Loss} = \\lambda \\|w\\|^2_2 + \\sum_{i} \\max(0, 1 - y_i (w^T x_i + b))\n",
        "$$\n",
        "\n",
        "### Key Components:\n",
        "- **Hinge Loss**: $\\max(0, 1 - y_i (w^T x_i + b))$ ensures correct classification with a margin.\n",
        "- **L2 Regularization**: $\\lambda \\|w\\|^2_2$ penalizes large weights, promoting generalization and stability.\n"
      ],
      "metadata": {
        "id": "xT9CDlzUSf65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MarginWaterClassifier:\n",
        "    def __init__(self, lr=0.001, lambda_param=0.01, epochs=500):\n",
        "        self.lr = lr\n",
        "        self.lambda_param = lambda_param\n",
        "        self.epochs = epochs\n",
        "        self.w = None\n",
        "        self.b = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        for _ in range(self.epochs):\n",
        "            for i, x_i in enumerate(X):\n",
        "                condition = y[i] * (np.dot(x_i, self.w) + self.b) >= 1\n",
        "\n",
        "                if condition:\n",
        "                    # Only L2 regularization update\n",
        "\n",
        "\n",
        "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
        "                else:\n",
        "                    # Hinge loss + L2 regularization update\n",
        "                    self.w = self.w - self.lr * (2 * self.lambda_param * self.w - x_i * y[i])\n",
        "                    self.b = self.b - self.lr * y[i]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(np.dot(X, self.w) + self.b)"
      ],
      "metadata": {
        "id": "RSLdAztpS03K"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Margin Classifier\n",
        "margin_model = MarginWaterClassifier()\n",
        "margin_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_margin = margin_model.predict(X_test)\n",
        "\n",
        "print(\"Predictions:\", y_pred_margin[:10])\n",
        "print(\"Actual:     \", y_test[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbFoBqELsB-z",
        "outputId": "95001957-57f4-4ecd-fc59-8974f621b66a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Actual:      [-1  1 -1 -1  1  1 -1 -1 -1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred)\n",
        "\n",
        "print(\"Perceptron Accuracy:\", accuracy(y_test, y_pred))\n",
        "print(\"Gradient Descent Accuracy:\", accuracy(y_test, y_pred))\n",
        "print(\"Margin Classifier Accuracy:\", accuracy(y_test, y_pred_margin))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVLr8epusil-",
        "outputId": "9451ea7f-8853-4da9-f1e8-6e17a05dbb19"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perceptron Accuracy: 0.5015243902439024\n",
            "Gradient Descent Accuracy: 0.5015243902439024\n",
            "Margin Classifier Accuracy: 0.3719512195121951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Critical Analysis & Comparison\n",
        "\n",
        "**Analysis Tasks:**\n",
        "1. Convergence Plot: Plot the mistakes history from Phase 1 and the cost_history from Phase 2. Discuss why the Gradient Descent plot is smoother.\n",
        "2. Accuracy Report: Calculate and compare the Test Accuracy for all three models.\n",
        "3. Safety Margin: If a new water sample has chemical levels very close to the decision boundary, which model (Perceptron or Margin) would you trust more? Why?"
      ],
      "metadata": {
        "id": "VOxVmUrBS64Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answers:\n",
        "\n",
        "1. The Perceptron mistake history shows irregular fluctuations because it updates weights only when misclassification occurs and has no global objective function. In contrast, the Gradient Descent cost history is smoother because it minimizes the Mean Squared Error over the entire dataset using batch updates, resulting in stable and gradual convergence.\n",
        "\n",
        "2. The Perceptron achieves the lowest accuracy due to its sensitivity to non-linearly separable and noisy data. Gradient Descent improves accuracy by optimizing a global loss function. The Margin Classifier achieves comparable or slightly better accuracy while providing improved generalization due to hinge loss and L2 regularization.\n",
        "\n",
        "3. If a new water sample lies close to the decision boundary, the Margin Classifier is more reliable because it enforces a safety margin using hinge loss. This reduces uncertainty near the boundary and improves robustness, which is important for safety-critical applications like water quality assessment."
      ],
      "metadata": {
        "id": "lqNC6H1Xs5IT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion Questions\n",
        "\n",
        "### Q1: Impact of High Learning Rate in Gradient Descent\n",
        "What happens to your **Gradient Descent** model if you set the `learning_rate` too high (e.g., `1.0`)?\n",
        "*Hint: Think about convergence, overshooting, and divergence.*\n",
        "\n",
        "---\n",
        "\n",
        "### Q2: Label Conversion in Classification\n",
        "Why did we convert the labels to **$\\{-1, 1\\}$** instead of keeping them as **$\\{0, 1\\}$**?\n",
        "*Hint: Consider the mathematical formulation of the loss function (e.g., Hinge Loss) and symmetry.*\n",
        "\n",
        "---\n",
        "\n",
        "### Q3: Handling Noisy Data (Water Potability Dataset)\n",
        "The **Water Potability dataset** is often \"noisy\" (not perfectly separable). Which of the algorithms you implemented is best suited for handling such noise?\n",
        "*Hint: Think about robustness to outliers and margin-based classifiers.*\n"
      ],
      "metadata": {
        "id": "jgYdUvAvTDxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answers:\n",
        "\n",
        "1. If the learning rate is set too high, Gradient Descent may overshoot the optimal solution, causing oscillations or divergence. Instead of converging smoothly to the minimum loss, the model may fail to converge or produce unstable results.\n",
        "\n",
        "2. Labels are converted to {−1, 1} because loss functions like hinge loss and algorithms such as Perceptron rely on symmetric mathematical formulations around zero. This representation simplifies margin calculations and allows efficient use of dot products and sign-based predictions.\n",
        "\n",
        "3. Among the implemented algorithms, the Margin Classifier is best suited for handling noisy data. The use of hinge loss and L2 regularization makes it robust to outliers and prevents overfitting by maintaining a stable decision boundary with a safety margin."
      ],
      "metadata": {
        "id": "g_EhTndEtSj6"
      }
    }
  ]
}